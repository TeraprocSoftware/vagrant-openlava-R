#!/usr/bin/env Rscript
#
# kmeans-batch.R - Sample script that performs a kmeans analysis on the generated dataset.csv file
#
library("BatchJobs")
library(parallel)
library(ggplot2)

setwd("/vagrant/share") 

conf = BatchJobs:::getBatchJobsConf()
conf$cluster.functions = makeClusterFunctionsOpenLava("/vagrant/resources/ol.tmpl")

#options(BatchJobs.verbose = TRUE)

# read the dataset generated by gen-data.R - this is not really necessary for execution
# of the jobs, but we want to have the datasets available in the current R instance for
# when we generates the visualization at the end of the script.
data <- read.csv('/vagrant/resources/dataset.csv')

# this is the function that represents one job
parallel.function <- function(i) {
  set.seed(i)
  data <- read.csv('/vagrant/resources/dataset.csv')
  result <- kmeans( data, centers=5, nstart=40)
  result
}

# Make registry for batch job
reg <- makeRegistry(id="TPcalc")
ids <- batchMap(reg, fun=parallel.function, 1:3)
print(ids)
print(reg)

# submit the job to the OpenLava cluster
start.time <- Sys.time()
done <- submitJobs(reg)
waitForJobs(reg)

# Load results of all "done" jobs
done <- findDone(reg)
results <- reduceResultsList(reg, use.names='none')
temp.vector <- sapply( results, function(results) { results$tot.withinss } )
final_result <- results[[which.min(temp.vector)]]

end.time <- Sys.time()
time.taken <- end.time - start.time
print(time.taken)

# show the results
print(final_result$centers)

# plot the results
data$cluster = factor(final_result$cluster)
centers = as.data.frame(final_result$centers)
plot = ggplot(data=data, aes(x=x, y=y, color=cluster )) + geom_point() + geom_point(data=centers, aes(x=x,y=y, color='Center')) + geom_point(data=centers, aes(x=x,y=y, color='Center'), size=52, alpha=.3, show_guide=FALSE)
print(plot)

# clean up after the batch job by removing the registry
removeRegistry(reg,"no")